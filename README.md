ğŸ“„ RAG-based Document Question Answering App:

This project implements a Retrieval-Augmented Generation (RAG) system that allows users to upload a PDF document and ask natural language questions.

The system retrieves relevant document chunks using vector similarity search and generates context-grounded answers using a Large Language Model (LLM).

ğŸš€ Features

ğŸ“„ Upload any PDF document

âœ‚ï¸ Automatic text chunking

ğŸ” Semantic retrieval using embeddings + FAISS

ğŸ§  LLM-based generative answering

ğŸš« Hallucination-safe (answers restricted to retrieved context)

ğŸŒ Interactive Streamlit web interface

â˜ï¸ Deployable on Google Colab

ğŸ§  System Architecture (RAG Pipeline)
1. PDF Ingestion: Extract text from uploaded PDF.
2. Chunking: Split document text into overlapping chunks.
3. Embedding: Generate vector embeddings using a sentence-transformer model.
4. Vector Search: Store embeddings in FAISS and retrieve top-k relevant chunks for a query.
5. LLM Answering: Pass retrieved context to an LLM and generate a grounded, natural language answer.

ğŸ› ï¸ Tech Stack

Python

Streamlit â€“ UI

Sentence-Transformers â€“ Text embeddings

FAISS â€“ Vector similarity search

OpenAI API â€“ LLM-based answering

PyPDF â€“ PDF text extraction

Google Colab â€“ Execution & deployment

## How to Run
1. Install dependencies given in requirements.txt
2. Set the OpenAI API key
3. Run the Streamlit app using:
   streamlit run app.py

The full logic is implemented in the notebook, just copy the first cell from the notebook, and create app.py.
